{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e95289b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\T123011\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a7f26eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger(\"shap\").setLevel(logging.ERROR)\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\"Could not find the number of physical cores.*\",\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\"FEATURE_DEPENDENCE::independent.*\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "232c9a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ───────────────────────────────────────────────\n",
    "# 0. 定数設定\n",
    "# ───────────────────────────────────────────────\n",
    "TICKERS = [\"AAPL\", \"MSFT\", \"GOOG\", \"^VIX\", \"^GSPC\", \"^IXIC\", \"^TNX\"]\n",
    "START_DATE = \"2000-01-01\"\n",
    "END_DATE = \"2025-05-31\"\n",
    "TARGET_TICKER = \"AAPL\"\n",
    "TARGET_COL = \"Target_Dir\"\n",
    "TEST_SIZE = 0.2\n",
    "MAX_K = 15\n",
    "KMEANS_N = 100  # 背景データ要約サンプル数\n",
    "# 分析モード: \"effect_vs_interaction\" または \"obs_vs_intervention\"\n",
    "ANALYSIS_MODE = \"effect_vs_interaction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edf14589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. データ取得・前処理を開始...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  7 of 7 completed\n",
      "C:\\Users\\T123011\\AppData\\Local\\Temp\\ipykernel_44948\\4130747888.py:63: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{t}_Stoch_D\"] = k.rolling(3).mean()\n",
      "C:\\Users\\T123011\\AppData\\Local\\Temp\\ipykernel_44948\\4130747888.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{t}_ATR\"] = tr.rolling(14).mean()\n",
      "C:\\Users\\T123011\\AppData\\Local\\Temp\\ipykernel_44948\\4130747888.py:74: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{t}_Tenkan\"] = tenkan\n",
      "C:\\Users\\T123011\\AppData\\Local\\Temp\\ipykernel_44948\\4130747888.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{t}_Kijun\"] = kijun\n",
      "C:\\Users\\T123011\\AppData\\Local\\Temp\\ipykernel_44948\\4130747888.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{TARGET_TICKER}_Return_lag_{lag}\"] = data[f\"{TARGET_TICKER}_Return1D\"].shift(lag)\n",
      "C:\\Users\\T123011\\AppData\\Local\\Temp\\ipykernel_44948\\4130747888.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{TARGET_TICKER}_Return_lag_{lag}\"] = data[f\"{TARGET_TICKER}_Return1D\"].shift(lag)\n",
      "C:\\Users\\T123011\\AppData\\Local\\Temp\\ipykernel_44948\\4130747888.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{TARGET_TICKER}_Return_lag_{lag}\"] = data[f\"{TARGET_TICKER}_Return1D\"].shift(lag)\n",
      "C:\\Users\\T123011\\AppData\\Local\\Temp\\ipykernel_44948\\4130747888.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{TARGET_TICKER}_Return_lag_{lag}\"] = data[f\"{TARGET_TICKER}_Return1D\"].shift(lag)\n",
      "C:\\Users\\T123011\\AppData\\Local\\Temp\\ipykernel_44948\\4130747888.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{TARGET_TICKER}_Return_lag_{lag}\"] = data[f\"{TARGET_TICKER}_Return1D\"].shift(lag)\n",
      "C:\\Users\\T123011\\AppData\\Local\\Temp\\ipykernel_44948\\4130747888.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{TARGET_TICKER}_Volatility_{WINDOW_SIZE}d\"] = data[f\"{TARGET_TICKER}_Close\"].rolling(WINDOW_SIZE).std()\n",
      "C:\\Users\\T123011\\AppData\\Local\\Temp\\ipykernel_44948\\4130747888.py:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"{TARGET_TICKER}_High_vs_{WINDOW_SIZE}d\"] = data[f\"{TARGET_TICKER}_Close\"] / data[f\"{TARGET_TICKER}_Close\"].rolling(WINDOW_SIZE).max()\n",
      "C:\\Users\\T123011\\AppData\\Local\\Temp\\ipykernel_44948\\4130747888.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[TARGET_COL] = (target_ret > 0).astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習データ: 4159 件, テストデータ: 1040 件\n",
      "クラス分布: {1: 2213, 0: 1946}\n",
      "scale_pos_weight: 0.88\n"
     ]
    }
   ],
   "source": [
    "# ───────────────────────────────────────────────\n",
    "# 1. データ取得・前処理\n",
    "# ───────────────────────────────────────────────\n",
    "print(\"1. データ取得・前処理を開始...\")\n",
    "\n",
    "df = yf.download(\n",
    "    TICKERS,\n",
    "    start=START_DATE,\n",
    "    end=END_DATE,\n",
    "    interval=\"1d\",\n",
    "    auto_adjust=True,\n",
    "    threads=True,\n",
    ")\n",
    "\n",
    "open_p = df[\"Open\"].rename(columns=lambda c: f\"{c}_Open\")\n",
    "high_p = df[\"High\"].rename(columns=lambda c: f\"{c}_High\")\n",
    "low_p = df[\"Low\"].rename(columns=lambda c: f\"{c}_Low\")\n",
    "close_p = df[\"Close\"].rename(columns=lambda c: f\"{c}_Close\")\n",
    "vol_p = df.get(\"Volume\")\n",
    "if vol_p is not None:\n",
    "    vol_p = vol_p.rename(columns=lambda c: f\"{c}_Volume\")\n",
    "\n",
    "frames = [open_p, high_p, low_p, close_p]\n",
    "if vol_p is not None:\n",
    "    frames.append(vol_p)\n",
    "\n",
    "data = pd.concat(frames, axis=1).dropna()\n",
    "\n",
    "for t in TICKERS:\n",
    "    close_col = f\"{t}_Close\"\n",
    "    high_col = f\"{t}_High\"\n",
    "    low_col = f\"{t}_Low\"\n",
    "\n",
    "    data[f\"{t}_MA5\"] = data[close_col].rolling(5).mean()\n",
    "    data[f\"{t}_MA20\"] = data[close_col].rolling(20).mean()\n",
    "    data[f\"{t}_Volatility\"] = data[close_col].pct_change().rolling(14).std()\n",
    "    data[f\"{t}_Return1D\"] = data[close_col].pct_change()\n",
    "    d = data[close_col].diff()\n",
    "    up, dn = d.clip(lower=0), -d.clip(upper=0)\n",
    "    mean_up = up.rolling(14).mean()\n",
    "    mean_dn = dn.rolling(14).mean()\n",
    "    rs = mean_up / mean_dn.replace(0, np.nan)\n",
    "    data[f\"{t}_RSI\"] = 100 - 100 / (1 + rs)\n",
    "\n",
    "    ema12 = data[close_col].ewm(span=12, adjust=False).mean()\n",
    "    ema26 = data[close_col].ewm(span=26, adjust=False).mean()\n",
    "    macd = ema12 - ema26\n",
    "    signal = macd.ewm(span=9, adjust=False).mean()\n",
    "    data[f\"{t}_MACD\"] = macd\n",
    "    data[f\"{t}_MACD_signal\"] = signal\n",
    "    data[f\"{t}_MACD_hist\"] = macd - signal\n",
    "\n",
    "    ma20 = data[close_col].rolling(20).mean()\n",
    "    std20 = data[close_col].rolling(20).std()\n",
    "    upper = ma20 + 2 * std20\n",
    "    lower = ma20 - 2 * std20\n",
    "    data[f\"{t}_BB_pct\"] = (data[close_col] - lower) / (upper - lower)\n",
    "\n",
    "    high14 = data[high_col].rolling(14).max()\n",
    "    low14 = data[low_col].rolling(14).min()\n",
    "    k = (data[close_col] - low14) / (high14 - low14)\n",
    "    data[f\"{t}_Stoch_K\"] = k\n",
    "    data[f\"{t}_Stoch_D\"] = k.rolling(3).mean()\n",
    "\n",
    "    tr = pd.concat([\n",
    "        data[high_col] - data[low_col],\n",
    "        (data[high_col] - data[close_col].shift()).abs(),\n",
    "        (data[low_col] - data[close_col].shift()).abs(),\n",
    "    ], axis=1).max(axis=1)\n",
    "    data[f\"{t}_ATR\"] = tr.rolling(14).mean()\n",
    "\n",
    "    tenkan = (data[high_col].rolling(9).max() + data[low_col].rolling(9).min()) / 2\n",
    "    kijun = (data[high_col].rolling(26).max() + data[low_col].rolling(26).min()) / 2\n",
    "    data[f\"{t}_Tenkan\"] = tenkan\n",
    "    data[f\"{t}_Kijun\"] = kijun\n",
    "\n",
    "for lag in [1, 2, 3, 5, 10]:\n",
    "    data[f\"{TARGET_TICKER}_Return_lag_{lag}\"] = data[f\"{TARGET_TICKER}_Return1D\"].shift(lag)\n",
    "\n",
    "WINDOW_SIZE = 20\n",
    "data[f\"{TARGET_TICKER}_Volatility_{WINDOW_SIZE}d\"] = data[f\"{TARGET_TICKER}_Close\"].rolling(WINDOW_SIZE).std()\n",
    "data[f\"{TARGET_TICKER}_High_vs_{WINDOW_SIZE}d\"] = data[f\"{TARGET_TICKER}_Close\"] / data[f\"{TARGET_TICKER}_Close\"].rolling(WINDOW_SIZE).max()\n",
    "\n",
    "target_ret = data[f\"{TARGET_TICKER}_Close\"].pct_change().shift(-1)\n",
    "data[TARGET_COL] = (target_ret > 0).astype(int)\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "feature_names = list(data.drop(columns=TARGET_COL).columns)\n",
    "X = data[feature_names].values\n",
    "y = data[TARGET_COL].values\n",
    "\n",
    "split_idx = int(len(X) * (1 - TEST_SIZE))\n",
    "X_tr, X_te = X[:split_idx], X[split_idx:]\n",
    "y_tr, y_te = y[:split_idx], y[split_idx:]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tr = scaler.fit_transform(X_tr)\n",
    "X_te = scaler.transform(X_te)\n",
    "print(f\"学習データ: {len(X_tr)} 件, テストデータ: {len(X_te)} 件\")\n",
    "\n",
    "# クラス分布の確認と scale_pos_weight 設定\n",
    "class_counts = pd.Series(y_tr).value_counts()\n",
    "pos_weight = (\n",
    "    class_counts.get(0, 0) / class_counts.get(1, 1)\n",
    "    if class_counts.get(1, 0) != 0\n",
    "    else 1.0\n",
    ")\n",
    "print(f\"クラス分布: {class_counts.to_dict()}\")\n",
    "print(f\"scale_pos_weight: {pos_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95e61290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. モデル学習・予測を開始...\n",
      "Best params: {'subsample': 0.8, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 7, 'learning_rate': 0.1, 'gamma': 0.2, 'colsample_bytree': 1.0}\n",
      "Cross-val AUC: 0.512\n",
      "学習・予測が完了しました。\n"
     ]
    }
   ],
   "source": [
    "# ───────────────────────────────────────────────\n",
    "# 2. モデル学習・予測\n",
    "# ───────────────────────────────────────────────\n",
    "print(\"\\n2. モデル学習・予測を開始...\")\n",
    "\n",
    "param_dist = {\n",
    "    \"n_estimators\": [200, 300, 400, 500],\n",
    "    \"max_depth\": [3, 4, 5, 6, 7],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    \"subsample\": [0.6, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "    \"min_child_weight\": [1, 3, 5],\n",
    "    \"gamma\": [0, 0.1, 0.2],\n",
    "}\n",
    "base_model = XGBClassifier(\n",
    "    random_state=42,\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"logloss\",\n",
    "    scale_pos_weight=pos_weight,\n",
    ")\n",
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "search = RandomizedSearchCV(\n",
    "    base_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=0,\n",
    ")\n",
    "search.fit(X_tr, y_tr)\n",
    "best_params = search.best_params_\n",
    "best_score = search.best_score_\n",
    "print(f\"Best params: {best_params}\")\n",
    "print(f\"Cross-val AUC: {best_score:.3f}\")\n",
    "\n",
    "model = XGBClassifier(\n",
    "    **best_params,\n",
    "    random_state=42,\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"logloss\",\n",
    "    early_stopping_rounds=10,\n",
    "    scale_pos_weight=pos_weight,\n",
    ")\n",
    "model.fit(X_tr, y_tr, eval_set=[(X_te, y_te)], verbose=False)\n",
    "pred_proba = model.predict_proba(X_te)[:, 1]\n",
    "pred_label = (pred_proba > 0.5).astype(int)\n",
    "print(\"学習・予測が完了しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58219913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. モデル性能評価:\n",
      "Accuracy : 0.509\n",
      "AUC      : 0.496\n",
      "Precision: 0.540\n",
      "Recall   : 0.468\n",
      "F1       : 0.501\n"
     ]
    }
   ],
   "source": [
    "# ───────────────────────────────────────────────\n",
    "# 3. モデル性能評価\n",
    "# ───────────────────────────────────────────────\n",
    "print(\"\\n3. モデル性能評価:\")\n",
    "acc = accuracy_score(y_te, pred_label)\n",
    "auc = roc_auc_score(y_te, pred_proba)\n",
    "precision = precision_score(y_te, pred_label)\n",
    "recall = recall_score(y_te, pred_label)\n",
    "f1 = f1_score(y_te, pred_label)\n",
    "print(f\"Accuracy : {acc:.3f}\")\n",
    "print(f\"AUC      : {auc:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall   : {recall:.3f}\")\n",
    "print(f\"F1       : {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4853c18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. SHAP / I-SHAP の計算を開始...（時間がかかる場合があります）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"C:\\Users\\T123011\\AppData\\Roaming\\Python\\Python312\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 247, in _count_physical_cores\n",
      "    cpu_count_physical = _count_physical_cores_win32()\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\T123011\\AppData\\Roaming\\Python\\Python312\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 299, in _count_physical_cores_win32\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\T123011\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\T123011\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\T123011\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "計算が完了しました。\n"
     ]
    }
   ],
   "source": [
    "# ───────────────────────────────────────────────\n",
    "# 4. SHAP / I-SHAP 計算\n",
    "# ───────────────────────────────────────────────\n",
    "print(\"\\n4. SHAP / I-SHAP の計算を開始...（時間がかかる場合があります）\")\n",
    "\n",
    "kmeans_obj = shap.kmeans(X_tr, KMEANS_N)\n",
    "background = getattr(kmeans_obj, \"data\", np.array(kmeans_obj))\n",
    "\n",
    "if ANALYSIS_MODE == \"effect_vs_interaction\":\n",
    "    explainer_shap = shap.TreeExplainer(\n",
    "        model,\n",
    "        data=background,\n",
    "        feature_perturbation=\"interventional\",\n",
    "    )\n",
    "    explainer_ishap = shap.TreeExplainer(\n",
    "        model,\n",
    "        data=background,\n",
    "        feature_perturbation=\"interventional\",\n",
    "    )\n",
    "else:  # \"obs_vs_intervention\"\n",
    "    explainer_shap = shap.TreeExplainer(\n",
    "        model,\n",
    "        data=background,\n",
    "        feature_perturbation=\"tree_path_dependent\",\n",
    "    )\n",
    "    explainer_ishap = shap.TreeExplainer(\n",
    "        model,\n",
    "        data=background,\n",
    "        feature_perturbation=\"interventional\",\n",
    "    )\n",
    "\n",
    "import contextlib\n",
    "with open(os.devnull, \"w\") as fnull:\n",
    "    with contextlib.redirect_stderr(fnull):\n",
    "        shap_vals = explainer_shap.shap_values(X_te)\n",
    "        if ANALYSIS_MODE == \"effect_vs_interaction\":\n",
    "            ishap_vals = explainer_ishap.shap_interaction_values(X_te)\n",
    "        else:\n",
    "            ishap_vals = explainer_ishap.shap_values(X_te)\n",
    "\n",
    "if isinstance(shap_vals, list):\n",
    "    shap_vals = shap_vals[0]\n",
    "if isinstance(ishap_vals, list):\n",
    "    ishap_vals = ishap_vals[0]\n",
    "\n",
    "mean_abs_shap = np.abs(shap_vals).mean(axis=0)\n",
    "if ANALYSIS_MODE == \"effect_vs_interaction\":\n",
    "    mean_abs_ishap = np.abs(ishap_vals).sum(axis=2).mean(axis=0)\n",
    "else:\n",
    "    mean_abs_ishap = np.abs(ishap_vals).mean(axis=0)\n",
    "\n",
    "baseline = X_tr.mean(axis=0)\n",
    "orig_preds = pred_proba\n",
    "\n",
    "def fidelity(mask_idx):\n",
    "    if len(mask_idx) == 0:\n",
    "        return 0.0\n",
    "    Xc = X_te.copy()\n",
    "    Xc[:, mask_idx] = baseline[mask_idx]\n",
    "    return np.mean(np.abs(orig_preds - model.predict_proba(Xc)[:, 1]))\n",
    "\n",
    "results = []\n",
    "for K in range(MAX_K):\n",
    "    top_sh = np.argsort(mean_abs_shap)[-K:] if K > 0 else []\n",
    "    comp_s = mean_abs_shap[top_sh].sum() / mean_abs_shap.sum() if mean_abs_shap.sum() > 0 else 0\n",
    "    bot_sh = [i for i in range(len(mean_abs_shap)) if i not in top_sh]\n",
    "    fid_s = fidelity(bot_sh)\n",
    "\n",
    "    top_ish = np.argsort(mean_abs_ishap)[-K:] if K > 0 else []\n",
    "    comp_i = mean_abs_ishap[top_ish].sum() / mean_abs_ishap.sum() if mean_abs_ishap.sum() > 0 else 0\n",
    "    bot_ish = [i for i in range(len(mean_abs_ishap)) if i not in top_ish]\n",
    "    fid_i = fidelity(bot_ish)\n",
    "\n",
    "    results.append({\n",
    "        \"K\": K,\n",
    "        \"Comp_SHAP\": comp_s,\n",
    "        \"Fid_SHAP\": fid_s,\n",
    "        \"Comp_ISHAP\": comp_i,\n",
    "        \"Fid_ISHAP\": fid_i,\n",
    "    })\n",
    "\n",
    "df_res = pd.DataFrame(results)\n",
    "print(\"計算が完了しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c620e9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. グラフとCSVファイルを出力します...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     49\u001b[39m plt.savefig(\u001b[33m\"\u001b[39m\u001b[33mprediction_vs_truth.png\u001b[39m\u001b[33m\"\u001b[39m, dpi=\u001b[32m300\u001b[39m)\n\u001b[32m     50\u001b[39m plt.close()\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m csv_path = os.path.join(os.path.dirname(\u001b[34;43m__file__\u001b[39;49m), \u001b[33m\"\u001b[39m\u001b[33mtop_shap_ishap_elements.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(csv_path, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     54\u001b[39m     writer = csv.writer(f)\n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# ───────────────────────────────────────────────\n",
    "# 5. 結果表示 & グラフ保存\n",
    "# ───────────────────────────────────────────────\n",
    "print(\"\\n5. グラフとCSVファイルを出力します...\")\n",
    "\n",
    "k_list = [5, 10, MAX_K - 1]\n",
    "df_plot = df_res[df_res[\"K\"].isin(k_list)]\n",
    "x = np.arange(len(k_list))\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "bar_w = 0.35\n",
    "plt.bar(x - bar_w / 2, df_plot[\"Fid_SHAP\"], bar_w, label=\"Fidelity (SHAP)\")\n",
    "plt.bar(x + bar_w / 2, df_plot[\"Fid_ISHAP\"], bar_w, label=\"Fidelity (I-SHAP)\")\n",
    "plt.xticks(x, k_list)\n",
    "plt.xlabel(\"K (Top features count)\")\n",
    "plt.ylabel(\"Fidelity (MAE)\")\n",
    "plt.title(\"Fidelity Comparison: SHAP vs I-SHAP\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"shap-ishap_fidelity_bar.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8), sharex=True)\n",
    "ax1.plot(df_res[\"K\"], df_res[\"Comp_SHAP\"], \"-o\", label=\"Completeness (SHAP)\")\n",
    "ax1.plot(df_res[\"K\"], df_res[\"Comp_ISHAP\"], \"--s\", label=\"Completeness (I-SHAP)\")\n",
    "ax1.set_ylabel(\"Completeness\")\n",
    "ax1.legend(); ax1.grid(True)\n",
    "\n",
    "ax2.plot(df_res[\"K\"], df_res[\"Fid_SHAP\"], \"-o\", label=\"Fidelity (SHAP)\")\n",
    "ax2.plot(df_res[\"K\"], df_res[\"Fid_ISHAP\"], \"--s\", label=\"Fidelity (I-SHAP)\")\n",
    "ax2.set_ylabel(\"Fidelity (MAE)\")\n",
    "ax2.set_xlabel(\"K (Top features count)\")\n",
    "ax2.legend(); ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"completeness_fidelity_vs_k.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "pred_tr = model.predict_proba(X_tr)[:, 1]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.arange(len(y)), y, color=\"gray\", label=\"Truth\")\n",
    "plt.plot(np.arange(len(pred_tr)), pred_tr, color=\"green\", label=\"Prediction (Train)\")\n",
    "plt.plot(np.arange(len(pred_tr), len(y)), pred_proba, color=\"red\", label=\"Prediction (Test)\")\n",
    "plt.axvline(x=split_idx, color=\"blue\", linestyle=\"--\", label=\"Train/Test Split\")\n",
    "plt.title(f\"Prediction vs Truth (Test AUC: {auc:.3f})\")\n",
    "plt.xlabel(\"Time index\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "plt.savefig(\"prediction_vs_truth.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "csv_path = os.path.join(os.path.dirname(__file__), \"top_shap_ishap_elements.csv\")\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"K\", \"SHAP_Top_Features\", \"I-SHAP_Top_Features\"])\n",
    "    for K in range(MAX_K):\n",
    "        top_sh = np.argsort(mean_abs_shap)[-K:][::-1] if K > 0 else []\n",
    "        top_ish = np.argsort(mean_abs_ishap)[-K:][::-1] if K > 0 else []\n",
    "        sh_feats = \"; \".join(feature_names[i] for i in top_sh)\n",
    "        ish_feats = \"; \".join(feature_names[i] for i in top_ish)\n",
    "        writer.writerow([K, sh_feats, ish_feats])\n",
    "\n",
    "print(\"すべての処理が完了しました。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
