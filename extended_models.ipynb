{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 拡張時系列モデル評価\n",
    "複数のモデルを試し、途中経過を表示しながら精度を比較します。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    prophet_available = True\n",
    "except Exception as e:\n",
    "    Prophet = None\n",
    "    prophet_available = False\n",
    "    prophet_error = e\n",
    "\n",
    "try:\n",
    "    from statsmodels.tsa.api import ExponentialSmoothing, ARIMA\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    statsmodels_available = True\n",
    "except Exception as e:\n",
    "    ExponentialSmoothing = ARIMA = SARIMAX = None\n",
    "    statsmodels_available = False\n",
    "    statsmodels_error = e\n",
    "\n",
    "try:\n",
    "    from tbats import TBATS\n",
    "    tbats_available = True\n",
    "except Exception as e:\n",
    "    TBATS = None\n",
    "    tbats_available = False\n",
    "    tbats_error = e\n",
    "\n",
    "try:\n",
    "    from neuralprophet import NeuralProphet\n",
    "    neuralprophet_available = True\n",
    "except Exception as e:\n",
    "    NeuralProphet = None\n",
    "    neuralprophet_available = False\n",
    "    neuralprophet_error = e\n",
    "\n",
    "# 深層学習系: TensorFlow/Keras を使う\n",
    "try:\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, GRU, Bidirectional\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    tf_available = True\n",
    "except Exception as e:\n",
    "    Sequential = Dense = SimpleRNN = LSTM = GRU = Bidirectional = EarlyStopping = None\n",
    "    tf_available = False\n",
    "    tf_error = e\n",
    "\n",
    "try:\n",
    "    from gluonts.dataset.common import ListDataset\n",
    "    from gluonts.model.deepar import DeepAREstimator\n",
    "    from gluonts.model.deep_factor import DeepFactorEstimator\n",
    "    from gluonts.model.seq2seq import MQCNNEstimator\n",
    "    from gluonts.mx.trainer import Trainer\n",
    "    gluonts_available = True\n",
    "except Exception as e:\n",
    "    DeepAREstimator = DeepFactorEstimator = MQCNNEstimator = Trainer = None\n",
    "    gluonts_available = False\n",
    "    gluonts_error = e\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('1. データ取得と前処理 ...')\n",
    "TICKERS = ['AAPL', '^GSPC']\n",
    "START_DATE = '2010-01-01'\n",
    "END_DATE = '2023-12-31'\n",
    "TARGET_TICKER = 'AAPL'\n",
    "TARGET_COL = f'{TARGET_TICKER}_Close'\n",
    "TEST_SIZE_RATIO = 0.3\n",
    "\n",
    "# 株価データの取得\n",
    "df = yf.download(TICKERS, start=START_DATE, end=END_DATE, progress=False)\n",
    "\n",
    "# 特徴量作成\n",
    "data_for_tabular = pd.concat([df['Close'].rename(columns=lambda c: f\"{c}_Close\"),\n",
    "                             df['Volume'].rename(columns=lambda c: f\"{c}_Volume\")], axis=1)\n",
    "for t in TICKERS:\n",
    "    data_for_tabular[f'{t}_Return1D'] = data_for_tabular[f'{t}_Close'].pct_change()\n",
    "    data_for_tabular[f'{t}_MA20'] = data_for_tabular[f'{t}_Close'].rolling(20).mean()\n",
    "    data_for_tabular[f'{t}_Volatility'] = data_for_tabular[f'{t}_Close'].pct_change().rolling(20).std()\n",
    "\n",
    "data_for_tabular['Target_Dir'] = (data_for_tabular[TARGET_COL].shift(-1) > data_for_tabular[TARGET_COL]).astype(int)\n",
    "data_for_tabular = data_for_tabular.dropna()\n",
    "if '^GSPC_Volume' in data_for_tabular.columns:\n",
    "    data_for_tabular = data_for_tabular.drop(columns='^GSPC_Volume')\n",
    "\n",
    "split_idx = int(len(data_for_tabular) * (1 - TEST_SIZE_RATIO))\n",
    "X = data_for_tabular.drop(columns='Target_Dir')\n",
    "y = data_for_tabular['Target_Dir']\n",
    "X_tr, X_te = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_tr, y_te = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "scaler = StandardScaler()\n",
    "X_tr_scaled = scaler.fit_transform(X_tr)\n",
    "X_te_scaled = scaler.transform(X_te)\n",
    "\n",
    "ts_df = df['Close'][[TARGET_TICKER]].reset_index()\n",
    "ts_df.columns = ['ds', 'y']\n",
    "ts_train, ts_test = ts_df.iloc[:split_idx], ts_df.iloc[split_idx:]\n",
    "\n",
    "y_true_dir = (ts_test['y'].values > ts_test['y'].shift(1).values).astype(int)[1:]\n",
    "results_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('2. テーブルデータ系モデルのチューニングと評価 ...')\n",
    "pos_weight = (y_tr == 0).sum() / (y_tr == 1).sum()\n",
    "cv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "lr_params = {'C': [0.1, 1, 10]}\n",
    "xgb_params = {'n_estimators': [100, 200], 'max_depth': [3, 5]}\n",
    "lgbm_params = {'n_estimators': [100, 200], 'num_leaves': [31, 63]}\n",
    "cat_params = {'depth': [4, 6], 'learning_rate': [0.03, 0.1]}\n",
    "\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression(max_iter=1000, class_weight='balanced'), lr_params),\n",
    "    ('XGBoost', XGBClassifier(random_state=42, scale_pos_weight=pos_weight), xgb_params),\n",
    "    ('LightGBM', LGBMClassifier(random_state=42, is_unbalance=True), lgbm_params),\n",
    "    ('CatBoost', CatBoostClassifier(random_state=42, scale_pos_weight=pos_weight, verbose=0), cat_params),\n",
    "]\n",
    "\n",
    "for name, base_model, params in models:\n",
    "    print(f'{name} をチューニング中...')\n",
    "    search = RandomizedSearchCV(base_model, params, n_iter=2, cv=cv, scoring='accuracy', random_state=42, verbose=1)\n",
    "    search.fit(X_tr_scaled, y_tr)\n",
    "    best_model = search.best_estimator_\n",
    "    pred = best_model.predict(X_te_scaled)\n",
    "    acc = accuracy_score(y_te, pred)\n",
    "    try:\n",
    "        proba = best_model.predict_proba(X_te_scaled)[:,1]\n",
    "        auc = roc_auc_score(y_te, proba)\n",
    "    except Exception:\n",
    "        auc = np.nan\n",
    "    results_list.append({'Model': name, 'Accuracy': acc, 'AUC': auc})\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('3. 一般的な時系列モデルの評価 ...')\n",
    "if statsmodels_available:\n",
    "    hw_model = ExponentialSmoothing(ts_train['y'], trend='add', seasonal='add', seasonal_periods=252).fit()\n",
    "    hw_pred = hw_model.forecast(len(ts_test))\n",
    "    hw_dir = (hw_pred.values > ts_test['y'].shift(1).values).astype(int)[1:]\n",
    "    acc_hw = accuracy_score(y_true_dir, hw_dir)\n",
    "    results_list.append({'Model': 'Holt-Winters', 'Accuracy': acc_hw, 'AUC': np.nan})\n",
    "\n",
    "    best_aic = float('inf')\n",
    "    best_order = None\n",
    "    for p in range(3):\n",
    "        for q in range(3):\n",
    "            try:\n",
    "                m = ARIMA(ts_train['y'], order=(p,1,q)).fit()\n",
    "                if m.aic < best_aic:\n",
    "                    best_aic = m.aic\n",
    "                    best_order = (p,1,q)\n",
    "            except Exception:\n",
    "                continue\n",
    "    if best_order:\n",
    "        arima_model = ARIMA(ts_train['y'], order=best_order).fit()\n",
    "        arima_pred = arima_model.forecast(len(ts_test))\n",
    "        arima_dir = (arima_pred.values > ts_test['y'].shift(1).values).astype(int)[1:]\n",
    "        acc_arima = accuracy_score(y_true_dir, arima_dir)\n",
    "        results_list.append({'Model': f'ARIMA{best_order}', 'Accuracy': acc_arima, 'AUC': np.nan})\n",
    "\n",
    "    best_aic = float('inf')\n",
    "    best_order = None\n",
    "    for p in range(2):\n",
    "        for q in range(2):\n",
    "            try:\n",
    "                m = SARIMAX(ts_train['y'], order=(p,1,q), seasonal_order=(p,0,q,252)).fit(disp=False)\n",
    "                if m.aic < best_aic:\n",
    "                    best_aic = m.aic\n",
    "                    best_order = (p,1,q)\n",
    "            except Exception:\n",
    "                continue\n",
    "    if best_order:\n",
    "        sarimax_model = SARIMAX(ts_train['y'], order=best_order, seasonal_order=(best_order[0],0,best_order[2],252)).fit(disp=False)\n",
    "        sarimax_pred = sarimax_model.forecast(len(ts_test))\n",
    "        sarimax_dir = (sarimax_pred.values > ts_test['y'].shift(1).values).astype(int)[1:]\n",
    "        acc_sarimax = accuracy_score(y_true_dir, sarimax_dir)\n",
    "        results_list.append({'Model': f'SARIMAX{best_order}', 'Accuracy': acc_sarimax, 'AUC': np.nan})\n",
    "else:\n",
    "    print(f'statsmodels の読み込みに失敗: {statsmodels_error}')\n",
    "\n",
    "if tbats_available:\n",
    "    tbats_est = TBATS(seasonal_periods=(252,))\n",
    "    tbats_model = tbats_est.fit(ts_train['y'])\n",
    "    tbats_pred = tbats_model.forecast(steps=len(ts_test))\n",
    "    tbats_dir = (tbats_pred > ts_test['y'].shift(1).values).astype(int)[1:]\n",
    "    acc_tbats = accuracy_score(y_true_dir, tbats_dir)\n",
    "    results_list.append({'Model': 'TBATS', 'Accuracy': acc_tbats, 'AUC': np.nan})\n",
    "else:\n",
    "    print(f'TBATS の読み込みに失敗: {tbats_error}')\n",
    "\n",
    "if prophet_available:\n",
    "    prophet_model = Prophet()\n",
    "    prophet_model.fit(ts_train)\n",
    "    future = prophet_model.make_future_dataframe(periods=len(ts_test))\n",
    "    fcst = prophet_model.predict(future)\n",
    "    prophet_pred = fcst['yhat'].iloc[-len(ts_test):]\n",
    "    prophet_dir = (prophet_pred.values > ts_test['y'].shift(1).values).astype(int)[1:]\n",
    "    acc_prophet = accuracy_score(y_true_dir, prophet_dir)\n",
    "    results_list.append({'Model': 'Prophet', 'Accuracy': acc_prophet, 'AUC': np.nan})\n",
    "else:\n",
    "    print(f'Prophet の読み込みに失敗: {prophet_error}')\n",
    "\n",
    "if neuralprophet_available:\n",
    "    nprophet = NeuralProphet()\n",
    "    nprophet.fit(ts_train, freq='D')\n",
    "    forecast = nprophet.predict(nprophet.make_future_dataframe(ts_train, periods=len(ts_test)))\n",
    "    nprophet_pred = forecast['yhat1'].iloc[-len(ts_test):]\n",
    "    nprophet_dir = (nprophet_pred.values > ts_test['y'].shift(1).values).astype(int)[1:]\n",
    "    acc_nprophet = accuracy_score(y_true_dir, nprophet_dir)\n",
    "    results_list.append({'Model': 'NeuralProphet', 'Accuracy': acc_nprophet, 'AUC': np.nan})\n",
    "else:\n",
    "    print(f'NeuralProphet の読み込みに失敗: {neuralprophet_error}')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('4. 深層学習系モデルの評価 ...')\n",
    "if tf_available:\n",
    "    # シンプルな LSTM モデル\n",
    "    def build_lstm():\n",
    "        model = Sequential([\n",
    "            LSTM(32, input_shape=(1, 1)),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    X_dl = ts_train['y'].pct_change().dropna().values.reshape(-1,1,1)\n",
    "    y_dl = (ts_train['y'].pct_change().shift(-1).dropna() > 0).astype(int).values\n",
    "    split = int(len(X_dl)*0.8)\n",
    "    X_tr_dl, X_te_dl = X_dl[:split], X_dl[split:]\n",
    "    y_tr_dl, y_te_dl = y_dl[:split], y_dl[split:]\n",
    "\n",
    "    lstm = build_lstm()\n",
    "    lstm.fit(X_tr_dl, y_tr_dl, epochs=2, verbose=1)\n",
    "    preds = lstm.predict(X_te_dl).ravel()\n",
    "    preds_bin = (preds > 0.5).astype(int)\n",
    "    acc_lstm = accuracy_score(y_te_dl, preds_bin)\n",
    "    results_list.append({'Model': 'LSTM', 'Accuracy': acc_lstm, 'AUC': roc_auc_score(y_te_dl, preds)})\n",
    "else:\n",
    "    print(f'TensorFlow の読み込みに失敗: {tf_error}')\n",
    "\n",
    "if gluonts_available:\n",
    "    train_ds = ListDataset([{'start': ts_train['ds'].iloc[0], 'target': ts_train['y'].values}], freq='D')\n",
    "    test_ds = ListDataset([{'start': ts_test['ds'].iloc[0], 'target': ts_test['y'].values}], freq='D')\n",
    "    trainer = Trainer(epochs=1)\n",
    "    deepar_est = DeepAREstimator(freq='D', prediction_length=len(ts_test), trainer=trainer)\n",
    "    deepar_model = deepar_est.train(train_ds)\n",
    "    for entry, forecast in zip(test_ds, deepar_model.predict(test_ds)):\n",
    "        deepar_pred = forecast.mean\n",
    "    deepar_dir = (deepar_pred > ts_test['y'].shift(1).values).astype(int)[1:]\n",
    "    acc_deepar = accuracy_score(y_true_dir, deepar_dir)\n",
    "    results_list.append({'Model': 'DeepAR', 'Accuracy': acc_deepar, 'AUC': np.nan})\n",
    "else:\n",
    "    print(f'GluonTS の読み込みに失敗: {gluonts_error}')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('\n",
    "5. 評価結果')\n",
    "res_df = pd.DataFrame(results_list)\n",
    "print(res_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
